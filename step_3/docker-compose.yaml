version: '3'
services:

  jupyter1:
    container_name: jupyter1
    image: jupyter/scipy-notebook:notebook-7.0.3
    hostname: jupyter1
    volumes:
      - ./notebooks:/home/jovyan/
    ports:
      - "8888:8888"
    environment:
      PYTHONUNBUFFERED: 1 
      CLUSTER_ID: 'BDb4EWyiS1GjcEKCew2HvQ'
    command: "start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"

  kafka1:
    image: confluentinc/cp-kafka
    container_name: kafka1
    hostname: kafka1
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://kafka1:9092,CONTROLLER://kafka1:9093'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka1:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      CLUSTER_ID: 'BDb4EWyiS1GjcEKCew2HvQ'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

  kafka2:
    image: confluentinc/cp-kafka
    container_name: kafka2
    hostname: kafka2
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://kafka2:9092,CONTROLLER://kafka2:9093'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka2:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      CLUSTER_ID: 'BDb4EWyiS1GjcEKCew2HvQ'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

  kafka3:
    image: confluentinc/cp-kafka
    container_name: kafka3
    hostname: kafka3
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://kafka3:9092,CONTROLLER://kafka3:9093'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka3:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      CLUSTER_ID: 'BDb4EWyiS1GjcEKCew2HvQ'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop1
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka1:9092,kafka2:9092,kafka3:9092"
    depends_on: 
      - kafka1

  rabbitmq:
    image: rabbitmq:3.11-management # Image includes the management UI plugin
    container_name: rabbitmq1
    hostname: rabbitmq1
    ports:
      - "5672:5672" # AMQP port
      - "15672:15672" # Management UI port
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq # Persist data
    environment:
      # Default user/pass: guest/guest (use proper credentials in production)
      RABBITMQ_DEFAULT_USER: user
      RABBITMQ_DEFAULT_PASS: password

  # Optional: Add services for the RabbitMQ producers/consumers if containerizing them
  producer-sensor-rabbit:
    build: ./producer-sensor-rabbit # Path to directory containing Dockerfile and script
    container_name: producer-sensor-rabbit
    depends_on:
      - rabbitmq
    # Add environment variables if needed (e.g., RABBITMQ_HOST=rabbitmq1)

  producer-activity-rabbit:
    build: ./producer-activity-rabbit
    container_name: producer-activity-rabbit
    depends_on:
      - rabbitmq

  processor-sink-rabbit:
    build: ./processor-sink-rabbit
    container_name: processor-sink-rabbit
    volumes: # Map volume to get CSV out
      - ./notebooks:/data # Write CSV to /data/processed_data_rabbit.csv
    depends_on:
      - rabbitmq

volumes:
  rabbitmq_data: